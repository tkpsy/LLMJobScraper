import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

from src.scrapers.html_scraper import HTMLScraper
from src.processors.job_extractor import JobExtractor
from src.processors.job_matcher import JobMatcher
from src.models.user_profile import UserProfile
from src.utils.config import (
    SCRAPING_CONFIG, MATCHING_CONFIG, AUTO_EXECUTION_CONFIG, 
    USER_PROFILE_CONFIG, EXECUTION_CONFIG, OUTPUT_CONFIG
)

class CrowdWorksCategoryExplorer:
    """ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®CrowdWorksæ¡ˆä»¶æ¢ç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.html_scraper = HTMLScraper()
        self.job_extractor = JobExtractor()
        self.job_matcher = JobMatcher()
        self.categories_file = Path("categories.json")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è·¡
        self.saved_files = {
            'html_files': [],
            'job_files': [],
            'match_files': [],
            'screenshot_files': []
        }
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        self.user_profile = UserProfile(
            skills=USER_PROFILE_CONFIG["skills"],
            experience_years=USER_PROFILE_CONFIG["experience_years"],
            preferred_categories=USER_PROFILE_CONFIG["preferred_categories"],
            preferred_work_type=USER_PROFILE_CONFIG["preferred_work_type"],
            min_budget=USER_PROFILE_CONFIG["min_budget"],
            description=USER_PROFILE_CONFIG["description"]
        )
    
    def load_categories(self) -> Dict:
        """ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
        if not self.categories_file.exists():
            print("ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã¾ãšã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")
            return {}
        
        with open(self.categories_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def find_category_by_name(self, categories: Dict, main_category_name: str, subcategory_name: Optional[str] = None) -> Optional[Dict]:
        """ã‚«ãƒ†ã‚´ãƒªåã‹ã‚‰è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œç´¢"""
        for category in categories.get("main_categories", []):
            if category["name"] == main_category_name:
                if subcategory_name is None:
                    return category
                else:
                    # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œç´¢
                    for subcategory in category.get("subcategories", []):
                        if subcategory["name"] == subcategory_name:
                            return subcategory
        return None
    
    def scrape_category_jobs(self, category_url: str) -> List[Path]:
        """æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®æ¡ˆä»¶ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šã‚’æ›´æ–°
        original_url = SCRAPING_CONFIG["base_url"]
        SCRAPING_CONFIG["base_url"] = category_url
        
        try:
            if OUTPUT_CONFIG["console_output"]:
                print(f"ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {category_url}")
            
            # è¤‡æ•°ãƒšãƒ¼ã‚¸å¯¾å¿œã®ãƒã‚§ãƒƒã‚¯
            max_pages = EXECUTION_CONFIG.get("max_pages_per_category", 1)
            if max_pages > 1:
                # è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                html_files = self.html_scraper.save_html_with_pagination(
                    category_url=category_url, 
                    max_pages=max_pages
                )
            else:
                # å¾“æ¥ã®å˜ä¸€ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                html_file = self.html_scraper.save_html_single()
                html_files = [html_file]
            
            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²
            self.saved_files['html_files'].extend(html_files)
            
            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è¨˜éŒ²
            if EXECUTION_CONFIG["save_screenshots"]:
                for html_file in html_files:
                    timestamp = html_file.stem.replace('page_', '')
                    screenshot_file = html_file.parent / f'screenshot_{timestamp}.png'
                    if screenshot_file.exists():
                        self.saved_files['screenshot_files'].append(screenshot_file)
            
            return html_files
        
        except Exception as e:
            print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return []
        
        finally:
            # è¨­å®šã‚’å…ƒã«æˆ»ã™
            SCRAPING_CONFIG["base_url"] = original_url
    
    def extract_and_match_jobs(self, html_files: List[Path]) -> List:
        """è¤‡æ•°ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¡ˆä»¶ã‚’æŠ½å‡ºã—ã¦ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’è¡Œã†"""
        if OUTPUT_CONFIG["console_output"]:
            print("æ¡ˆä»¶æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
        
        all_jobs = []
        
        # è¤‡æ•°ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¡ˆä»¶ã‚’æŠ½å‡º
        for i, html_file in enumerate(html_files, 1):
            if OUTPUT_CONFIG["console_output"]:
                print(f"  ãƒ•ã‚¡ã‚¤ãƒ« {i}/{len(html_files)}: {html_file.name}")
            
            jobs = self.job_extractor.extract_jobs(html_file)
            all_jobs.extend(jobs)
            
            if OUTPUT_CONFIG["console_output"]:
                print(f"    æŠ½å‡ºä»¶æ•°: {len(jobs)}ä»¶")
        
        # é‡è¤‡æ¡ˆä»¶ã®é™¤å»
        unique_jobs = self._remove_duplicate_jobs(all_jobs)
        
        if OUTPUT_CONFIG["console_output"]:
            print(f"åˆè¨ˆæŠ½å‡ºä»¶æ•°: {len(all_jobs)}ä»¶")
            print(f"é‡è¤‡é™¤å»å¾Œ: {len(unique_jobs)}ä»¶")
        
        if not unique_jobs:
            if OUTPUT_CONFIG["console_output"]:
                print("æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
        
        # æ¡ˆä»¶ã‚’JSONã§ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        job_file = self.job_extractor.save_jobs_to_json(unique_jobs, timestamp)
        self.saved_files['job_files'].append(job_file)
        
        if OUTPUT_CONFIG["console_output"]:
            print(f"æŠ½å‡ºã•ã‚ŒãŸæ¡ˆä»¶æ•°: {len(unique_jobs)}ä»¶")
        
        # ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡
        if OUTPUT_CONFIG["console_output"]:
            print("æ¡ˆä»¶ã®ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
        matches = self.job_matcher.find_matching_jobs(
            user_profile=self.user_profile,
            min_score=MATCHING_CONFIG["min_score"],
            max_jobs=MATCHING_CONFIG["max_jobs"]
        )
        
        # æ¨è–¦æ¡ˆä»¶ã‚’JSONã§ä¿å­˜
        if matches:
            try:
                matching_result_file = self.job_matcher.save_matching_results(matches, self.user_profile)
                self.saved_files['match_files'].append(matching_result_file)
                
                if OUTPUT_CONFIG["console_output"]:
                    print(f"æ¨è–¦æ¡ˆä»¶ã‚’ {matching_result_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                    
            except Exception as e:
                if OUTPUT_CONFIG["console_output"]:
                    print(f"æ¨è–¦æ¡ˆä»¶ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒãƒƒãƒãƒ³ã‚°çµæœã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²
        match_files = list(Path("data/matches").glob("all_evaluations_*.csv"))
        if match_files:
            latest_match_file = max(match_files, key=lambda x: x.stat().st_mtime)
            if latest_match_file not in self.saved_files['match_files']:
                self.saved_files['match_files'].append(latest_match_file)
        
        return matches
    
    def _remove_duplicate_jobs(self, jobs: List) -> List:
        """é‡è¤‡æ¡ˆä»¶ã‚’é™¤å»ã™ã‚‹"""
        unique_jobs = []
        seen_titles = set()
        
        for job in jobs:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            job_key = (job.title, job.client_name)
            if job_key not in seen_titles:
                seen_titles.add(job_key)
                unique_jobs.append(job)
        
        return unique_jobs
    
    def display_matches(self, matches: List) -> None:
        """ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’è¡¨ç¤º"""
        if not OUTPUT_CONFIG["console_output"]:
            return
            
        if not matches:
            print("\næ¡ä»¶ã«åˆã†æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        print(f"\næ¨è–¦æ¡ˆä»¶ ({len(matches)}ä»¶):")
        print("=" * 60)
        
        for i, match in enumerate(matches, 1):
            job = match.job
            print(f"\n{i}. {job['title']}")
            print(f"   é–¢é€£åº¦ã‚¹ã‚³ã‚¢: {match.relevance_score:.1f}/100")
            print(f"   ã‚«ãƒ†ã‚´ãƒª: {job.get('category', 'æœªåˆ†é¡')}")
            print(f"   ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: {job.get('client_name', 'ä¸æ˜')}")
            
            if job.get('budget'):
                budget = job['budget']
                if budget.get('min_amount') and budget.get('max_amount'):
                    print(f"   äºˆç®—: {budget['min_amount']:,}å†† ï½ {budget['max_amount']:,}å†† ({budget['type']})")
                elif budget.get('min_amount'):
                    print(f"   äºˆç®—: {budget['min_amount']:,}å†† ({budget['type']})")
                else:
                    print(f"   äºˆç®—: ç›¸è«‡ ({budget['type']})")
            
            if job.get('deadline'):
                print(f"   æœŸé™: {job['deadline']}")
            
            if job.get('url'):
                print(f"   URL: {job['url']}")
            
            # èª¬æ˜æ–‡ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            description = job.get('description', '')
            if description:
                desc_preview = description[:100] + "..." if len(description) > 100 else description
                print(f"   èª¬æ˜: {desc_preview}")
            
            print("-" * 60)
    
    def display_saved_files_summary(self) -> None:
        """ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º"""
        if not OUTPUT_CONFIG["detailed_summary"]:
            return
            
        print("\n" + "=" * 60)
        print("ğŸ—‚ï¸  ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã«ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§")
        print("=" * 60)
        
        total_files = 0
        
        if self.saved_files['html_files']:
            print(f"\nğŸ“„ HTMLãƒ•ã‚¡ã‚¤ãƒ« ({len(self.saved_files['html_files'])}ä»¶):")
            for file_path in self.saved_files['html_files']:
                if OUTPUT_CONFIG["show_file_sizes"]:
                    file_size = file_path.stat().st_size / 1024  # KB
                    print(f"   - {file_path} ({file_size:.1f}KB)")
                else:
                    print(f"   - {file_path}")
                total_files += 1
        
        if self.saved_files['screenshot_files']:
            print(f"\nğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ ({len(self.saved_files['screenshot_files'])}ä»¶):")
            for file_path in self.saved_files['screenshot_files']:
                if OUTPUT_CONFIG["show_file_sizes"]:
                    file_size = file_path.stat().st_size / 1024  # KB
                    print(f"   - {file_path} ({file_size:.1f}KB)")
                else:
                    print(f"   - {file_path}")
                total_files += 1
        
        if self.saved_files['job_files']:
            print(f"\nğŸ“‹ æ¡ˆä»¶ãƒ‡ãƒ¼ã‚¿ (JSON) ({len(self.saved_files['job_files'])}ä»¶):")
            for file_path in self.saved_files['job_files']:
                if OUTPUT_CONFIG["show_file_sizes"]:
                    file_size = file_path.stat().st_size / 1024  # KB
                    print(f"   - {file_path} ({file_size:.1f}KB)")
                else:
                    print(f"   - {file_path}")
                total_files += 1
        
        if self.saved_files['match_files']:
            print(f"\nğŸ“Š ãƒãƒƒãƒãƒ³ã‚°çµæœ (CSV) ({len(self.saved_files['match_files'])}ä»¶):")
            for file_path in self.saved_files['match_files']:
                if OUTPUT_CONFIG["show_file_sizes"]:
                    file_size = file_path.stat().st_size / 1024  # KB
                    print(f"   - {file_path} ({file_size:.1f}KB)")
                else:
                    print(f"   - {file_path}")
                total_files += 1
        
        if total_files == 0:
            print("\nâš ï¸  ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            print(f"\nâœ… åˆè¨ˆ {total_files} ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        
        print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:")
        print("   - HTMLãƒ•ã‚¡ã‚¤ãƒ«: ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ç¢ºèªã§ãã¾ã™")
        print("   - JSONãƒ•ã‚¡ã‚¤ãƒ«: æ¡ˆä»¶ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒæ§‹é€ åŒ–ã•ã‚Œã¦ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")  
        print("   - CSVãƒ•ã‚¡ã‚¤ãƒ«: Excelã‚„ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã§é–‹ã„ã¦åˆ†æã§ãã¾ã™")
        print("=" * 60)
    
    def run(self):
        """è‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰"""
        if OUTPUT_CONFIG["console_output"]:
            print("CrowdWorks ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹æ¡ˆä»¶æ¢ç´¢ã‚·ã‚¹ãƒ†ãƒ ")
            print("=" * 60)
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        categories = self.load_categories()
        if not categories:
            return
        
        try:
            for i, target_config in enumerate(AUTO_EXECUTION_CONFIG["target_categories"]):
                if OUTPUT_CONFIG["console_output"]:
                    print(f"\nğŸ¯ å®Ÿè¡Œ {i+1}/{len(AUTO_EXECUTION_CONFIG['target_categories'])}: {target_config['description']}")
                
                # è¨­å®šã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œç´¢
                selected_category = self.find_category_by_name(
                    categories, 
                    target_config["main_category"], 
                    target_config.get("subcategory")
                )
                
                if selected_category is None:
                    print(f"âš ï¸  ã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_config['main_category']}")
                    if target_config.get("subcategory"):
                        print(f"    ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª: {target_config['subcategory']}")
                    continue
                
                if OUTPUT_CONFIG["console_output"]:
                    print(f"ğŸ“‚ å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª: {selected_category['name']}")
                
                # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                html_files = self.scrape_category_jobs(selected_category['url'])
                if not html_files:
                    continue
                
                # æ¡ˆä»¶æŠ½å‡ºã¨ãƒãƒƒãƒãƒ³ã‚°
                matches = self.extract_and_match_jobs(html_files)
                
                # çµæœè¡¨ç¤º
                self.display_matches(matches)
                
                # é€£ç¶šå®Ÿè¡Œã®å ´åˆã¯å¾…æ©Ÿ
                if (AUTO_EXECUTION_CONFIG["continuous_execution"] and 
                    i < len(AUTO_EXECUTION_CONFIG["target_categories"]) - 1):
                    delay = AUTO_EXECUTION_CONFIG["delay_between_categories"]
                    if OUTPUT_CONFIG["console_output"]:
                        print(f"\nâ³ æ¬¡ã®ã‚«ãƒ†ã‚´ãƒªã¾ã§ {delay} ç§’å¾…æ©Ÿ...")
                    time.sleep(delay)
        
        except KeyboardInterrupt:
            if OUTPUT_CONFIG["console_output"]:
                print("\n\nâš ï¸  ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        
        finally:
            # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º
            self.display_saved_files_summary()
            if OUTPUT_CONFIG["console_output"]:
                print("\nãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    explorer = CrowdWorksCategoryExplorer()
    explorer.run()

if __name__ == "__main__":
    main() 