from playwright.sync_api import sync_playwright
import time
from pathlib import Path
from datetime import datetime
from typing import List
from ..utils.config import SCRAPING_CONFIG

class HTMLScraper:
    """CrowdWorksã®HTMLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, save_dir: str = "data/html"):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
    
    def save_html_single(self) -> Path:
        """1å›åˆ†ã®HTMLä¿å­˜ã‚’è¡Œã†"""
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                viewport={'width': 1280, 'height': 800},
                accept_downloads=True,
                java_script_enabled=True,
                bypass_csp=True,
            )
            page = context.new_page()
            
            try:
                # è¨­å®šã‹ã‚‰æ¤œç´¢URLã‚’æ§‹ç¯‰
                params = "&".join([f"{k}={v}" for k, v in SCRAPING_CONFIG["search_params"].items()])
                url = f"{SCRAPING_CONFIG['base_url']}?{params}"
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Œå…¨ãªèª­ã¿è¾¼ã¿ã‚’å¾…æ©Ÿ
                page.wait_for_load_state('networkidle')
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦é…å»¶èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º
                page.evaluate("""
                    window.scrollTo(0, document.body.scrollHeight);
                    new Promise((resolve) => setTimeout(resolve, 2000));
                """)
                
                # ã•ã‚‰ã«å¾…æ©Ÿ
                time.sleep(3)
                
                # HTMLã‚’å–å¾—
                html_content = page.content()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç¾åœ¨æ™‚åˆ»ã‚’å«ã‚ã‚‹
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                save_path = self.save_dir / f'page_{timestamp}.html'
                
                # HTMLã‚’ä¿å­˜
                save_path.write_text(html_content, encoding='utf-8')
                print(f"Saved HTML to: {save_path}")
                
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚‚ä¿å­˜
                screenshot_path = self.save_dir / f'screenshot_{timestamp}.png'
                page.screenshot(path=screenshot_path, full_page=True)
                print(f"Saved screenshot to: {screenshot_path}")
                
                return save_path
                
            except Exception as e:
                print(f"Error occurred: {str(e)}")
                raise
            
            finally:
                browser.close()
    
    def save_html_multiple(self, times: int = 1, delay_seconds: int = 5) -> List[Path]:
        """æŒ‡å®šå›æ•°åˆ†ã®HTMLä¿å­˜ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1å›ï¼‰"""
        saved_files = []
        for i in range(times):
            print(f"\n=== å®Ÿè¡Œ {i+1}/{times} ===")
            saved_path = self.save_html_single()
            saved_files.append(saved_path)
            if i < times - 1:  # æœ€å¾Œã®å®Ÿè¡Œä»¥å¤–ã¯å¾…æ©Ÿ
                print(f"æ¬¡ã®å®Ÿè¡Œã¾ã§{delay_seconds}ç§’å¾…æ©Ÿ...")
                time.sleep(delay_seconds)
        return saved_files
    
    def save_html_with_pagination(self, category_url: str, max_pages: int = 3) -> List[Path]:
        """è¤‡æ•°ãƒšãƒ¼ã‚¸ã«è·¨ã£ãŸHTMLä¿å­˜ã‚’è¡Œã†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
        print(f"ğŸ“„ è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: æœ€å¤§{max_pages}ãƒšãƒ¼ã‚¸")
        
        saved_files = []
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
                viewport={'width': 1280, 'height': 800},
                accept_downloads=True,
                java_script_enabled=True,
                bypass_csp=True,
            )
            page = context.new_page()
            
            try:
                for page_num in range(1, max_pages + 1):
                    print(f"ğŸ” ãƒšãƒ¼ã‚¸ {page_num}/{max_pages} ã‚’å‡¦ç†ä¸­...")
                    
                    # ãƒšãƒ¼ã‚¸URLã‚’æ§‹ç¯‰
                    params = "&".join([f"{k}={v}" for k, v in SCRAPING_CONFIG["search_params"].items()])
                    if page_num == 1:
                        url = f"{category_url}?{params}"
                    else:
                        url = f"{category_url}?{params}&page={page_num}"
                    
                    print(f"  URL: {url}")
                    
                    # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                    page.goto(url, wait_until='networkidle', timeout=30000)
                    page.wait_for_load_state('networkidle')
                    
                    # æ—¢å­˜ã¨åŒã˜å¾…æ©Ÿå‡¦ç†
                    page.evaluate("""
                        window.scrollTo(0, document.body.scrollHeight);
                        new Promise((resolve) => setTimeout(resolve, 2000));
                    """)
                    time.sleep(3)
                    
                    # HTMLã‚’å–å¾—ã—ã¦ä¿å­˜
                    html_content = page.content()
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    save_path = self.save_dir / f'page_{timestamp}_p{page_num}.html'
                    
                    save_path.write_text(html_content, encoding='utf-8')
                    saved_files.append(save_path)
                    
                    print(f"  âœ… ä¿å­˜å®Œäº†: {save_path}")
                    
                    # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚‚ä¿å­˜
                    screenshot_path = self.save_dir / f'screenshot_{timestamp}_p{page_num}.png'
                    page.screenshot(path=screenshot_path, full_page=True)
                    
                    # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    if page_num < max_pages:
                        # æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        next_page_exists = self._check_next_page_exists(page, page_num + 1)
                        if not next_page_exists:
                            print(f"  âš ï¸  ãƒšãƒ¼ã‚¸ {page_num + 1} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚{page_num}ãƒšãƒ¼ã‚¸ã§çµ‚äº†ã—ã¾ã™ã€‚")
                            break
                        
                        # ãƒšãƒ¼ã‚¸é–“ã®å¾…æ©Ÿ
                        time.sleep(2)
                
                print(f"ğŸ‰ è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(saved_files)}ãƒšãƒ¼ã‚¸ä¿å­˜")
                return saved_files
                
            except Exception as e:
                print(f"âŒ è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                return saved_files  # é€”ä¸­ã¾ã§ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
            
            finally:
                browser.close()
    
    def _check_next_page_exists(self, page, next_page_num: int) -> bool:
        """æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
        try:
            # èª¿æŸ»ã§è¦‹ã¤ã‹ã£ãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨
            next_page_selectors = [
                'a:has-text("æ¬¡ã®ãƒšãƒ¼ã‚¸")',  # èª¿æŸ»ã§è¦‹ã¤ã‹ã£ãŸä¸»è¦ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
                f'a:has-text("{next_page_num}")',  # ãƒšãƒ¼ã‚¸ç•ªå·ãƒªãƒ³ã‚¯
                'nav a'  # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å†…ã®ãƒªãƒ³ã‚¯
            ]
            
            for selector in next_page_selectors:
                try:
                    elements = page.query_selector_all(selector)
                    for elem in elements:
                        text = elem.text_content().strip()
                        href = elem.get_attribute('href')
                        
                        # æ¬¡ã®ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ã¾ãŸã¯ãƒšãƒ¼ã‚¸ç•ªå·ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
                        if (('æ¬¡ã®ãƒšãƒ¼ã‚¸' in text) or 
                            (text == str(next_page_num)) or 
                            (href and f'page={next_page_num}' in href)):
                            return True
                except Exception:
                    continue
            
            return False
            
        except Exception:
            return False 